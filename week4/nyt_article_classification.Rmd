---
title: "NYTimes article classification"
author: "Your Name"
date: '`r Sys.time()`'
output:
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
library(here)
library(tm)
library(Matrix)
library(glmnet)
library(ROCR)
library(tidyverse)
library(broom)

theme_set(theme_bw())

knitr::opts_chunk$set(echo = TRUE)
```

# Part A

Read business and world articles into a single data frame.

```{r read-articles}

business <- read_tsv("business.tsv")
world <- read_tsv("world.tsv")
df <- rbind(business, world)

```

Create a Corpus from the article snippets using the `VectorSource()` and `Corpus()` functions from the `tm` package. Then create a DocumentTermMatrix from the snippet Corpus, removing punctuation and numbers and convert the DocumentTermMatrix to a sparseMatrix, required by cv.glmnet using the provided function. See the `DocumentTermMatrix()` documentation for more details, which has parameters for all of the punctuation and number parsing.

```{r create-sparse-dtm}

# helper function
dtm_to_sparse <- function(dtm) {
 sparseMatrix(i=dtm$i, j=dtm$j, x=dtm$v, dims=c(dtm$nrow, dtm$ncol), dimnames=dtm$dimnames)
}

vs <- VectorSource(df$snippet)
corpus <- Corpus(vs)
dtm <- DocumentTermMatrix(corpus, control = list(removePunctuation = TRUE, removeNumbers = TRUE))
sparse <- dtm_to_sparse(dtm)

```


# Part B

Create a train / test split

```{r create-train-test}

indices <- sample(nrow(articles), floor(nrow(articles) * .9))

trainX <- sparse[indices,]
trainY <- df$section[indices]
testX <- sparse[-indices,]
testY <- df$section[-indices]

```

Cross-validate on the training set using logistic regression with cv.glmnet, measuring auc. See documentation on `cv.glmnet()`, specifically the `family` and `type.measure` parameters.

```{r cross-validate}

cvfit <- cv.glmnet(trainX, trainY, family = "binomial", type.measure = "auc")

```

Evaluate performance for the best-fit (`lambda.min`) model by plotting the ROC curve and printing the accuracy and AUC.

```{r evaluate-best-model}

cm <- data.frame(actual = testY, pred = predict(cvfit, testX, s = "lambda.min", type = "response"))


pred <- prediction(cm$X1, testY)
perf_lr <- performance(pred, measure='tpr', x.measure='fpr')
plot(perf_lr)
performance(pred, 'auc')

cm2 <- data.frame(actual = testY, pred = predict(cvfit, testX, s = "lambda.min", type = "class"))
cm2 %>% summarize(accuracy = mean(actual == X1))
table(cm2)

```

# Part C

Count how many words have non-zero coefficients. Use the `coef()` and `tidy()` functions for `lambda.min`.

```{r count-nonzero-weights}

nrow(tidy(coef(cvfit, s = "lambda.min")))

```

Print the words with the top 10 heighest weights for the Business section. Do the same for the World section. Use the `coef()` and `tidy()` functions for `lambda.min`.


```{r show-top-words}

tidy(coef(cvfit, s = "lambda.min")) %>% arrange(desc(value)) %>% head(10)

tidy(coef(cvfit, s = "lambda.min")) %>% arrange(desc(value)) %>% tail(10)

```

Think about how this model would perform if you used it to classify data from today's newspaper compared to the data in the test set.
T
